{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b86374",
   "metadata": {},
   "source": [
    "This code is an implementation of a Generative Adversarial Network (GAN) using TensorFlow and Keras. The GAN is trained on a custom dataset of medical images to generate synthetic images resembling those in the dataset. Below is a description and explanation of the code:\n",
    "\n",
    "1. Importing Libraries: TensorFlow and necessary components from Keras are imported. Other standard libraries like `os`, `requests`, and `ZipFile` are imported for file manipulation and data downloading.\n",
    "\n",
    "2. Generator and Discriminator Architectures: Functions `build_generator` and `build_discriminator` define the architectures for the generator and discriminator networks, respectively. The generator takes a latent vector as input and outputs an image, while the discriminator takes an image as input and outputs a binary classification (real or fake).\n",
    "\n",
    "3. Building GAN: The function `build_gan` creates the GAN model by stacking the generator on top of the discriminator. During training, only the generator or discriminator is updated at a time, so the discriminator's weights are frozen when training the combined model.\n",
    "\n",
    "4. Loss Functions: Binary cross-entropy loss functions are defined for both the discriminator and generator. These are standard loss functions used in GANs.\n",
    "\n",
    "5. Optimizer Initialization: Adam optimizers are initialized for both the generator and discriminator.\n",
    "\n",
    "6. Data Loading and Preprocessing: A custom medical image dataset is downloaded from a URL using `requests` library, saved, and then extracted using `ZipFile`. The extracted dataset is then loaded and preprocessed using `ImageDataGenerator` from Keras.\n",
    "\n",
    "7. Training Function Call: The function `train_gan` is called with the generator, discriminator, GAN, dataset, batch size, and number of epochs as arguments to start training the GAN.\n",
    "\n",
    "8. Own Dataset Insertion: The dataset used in this code is a medical image dataset. However, the URL for the dataset (`\"https://www.example.com/dataset.zip\"`) is a placeholder. You would need to replace this URL with the actual URL of your dataset. Additionally, you can adjust the preprocessing steps in the `ImageDataGenerator` according to the requirements of your dataset.\n",
    "\n",
    "9. Training the GAN: The training function `train_gan` is not defined in the provided code snippet. You would need to define this function separately to train the GAN model. This function typically involves iterating over the dataset in batches, passing real and fake images through the discriminator, calculating losses, and updating the generator and discriminator weights accordingly.\n",
    "\n",
    "This code provides a foundation for training a GAN on a custom dataset of medical images. It outlines the necessary components such as model architectures, loss functions, optimizers, and data loading, but it requires additional implementation of the training loop to actually train the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97672bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(latent_dim,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(784, activation='tanh'),\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = Adam(learning_rate=1e-4)\n",
    "discriminator_optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator((28, 28, 1))\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "generator.compile(optimizer=generator_optimizer, loss=generator_loss)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss=discriminator_loss)\n",
    "gan.compile(optimizer=generator_optimizer, loss=generator_loss)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "# Path to save the dataset\n",
    "save_dir = \"./dataset\"\n",
    "# URL of the dataset zip file\n",
    "url = \"https://www.example.com/dataset.zip\"\n",
    "# Filename to save the zip file\n",
    "zip_filename = os.path.join(save_dir, \"dataset.zip\")\n",
    "\n",
    "# Check if the save directory exists, if not, create it\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Download the dataset zip file\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(\"Error downloading dataset from URL:\", e)\n",
    "else:\n",
    "    # Save the downloaded dataset zip file\n",
    "    with open(zip_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Extract the dataset from the zip file\n",
    "    with ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(save_dir)\n",
    "\n",
    "# Path to the extracted dataset\n",
    "dataset_path = os.path.join(save_dir, \"dataset\")\n",
    "\n",
    "# Load and preprocess the medical image dataset\n",
    "dataset = datagen.flow_from_directory(dataset_path, target_size=(28, 28), batch_size=128, class_mode='binary')\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, gan, dataset, batch_size=128, epochs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
